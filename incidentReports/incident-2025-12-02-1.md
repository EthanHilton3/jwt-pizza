# Incident: 2025-12-02 09-56-00

## Summary

Between the hour of 9:56 and 12:31 MST on 2025-12-02, all users encountered an error where no pizzas could be ordered. The event was triggered by a requested chaos test at 23:05 on 2025-12-01.

A bug in this code caused the endpoint for ordering pizzas to consistently fail. The event was detected by the Grafana Metrics dashboard. The team started working on the event by debugging the api calls. This severe incident affected 100% of users, as far as we can tell.

## Detection

This incident was detected at 12:28 MST on 2025-12-02 when the team lead manually looked at the Grafana Metrics Dashboard for the site.

Due to poorly designed alerting rules, no team member was alerted to the issue. None of the system's Grafana OnCall alerts were triggered. There was no alert to detect  The issue was undected until manually found.

New alerts for detecting pizza failures have be set up by Ethan Hilton so that if such failures occur again, they will be resolved immediately.

## Impact

For 2hrs 35 minutes, between 9:56 and 12:31 MST on 2025-12-02, our users experienced this incident.

This incident affected 2 customers (100% OF JWT Pizza USERS at the time), who experienced the failure to order a pizza.

0 support tickets were submitted.

## Timeline

All times are MST.

2025-12-01
- _23:05_ - JWT Pizza DevOps team requested a chaos test

2025-12-02
- _09:56_ - All user pizza orders started failing 
- _12:28_ - Team lead of JWT Pizza DevOps manually checks the Grafana Dashboard and notices all Pizza orders are failing
- _12:29_ - Team lead starts investigating failures by going to the production site and attempting to order a pizza
- _12:30_ - Team Lead uses the debug console in the browser to inspect the failed api calls
- _12:31_ - Team lead finds the cause of the issue in the endpoint response, navigating to the indicated URL to end the chaos test
- _12:32_ - Team lead inspects the Grafana Dashboard, verifies that pizza orders are now being fulfilled successfully and declares that the issue has been resolved
- _13:05_ - After reviewing the alerts rules for the project, new rules are added to monitor both the upper and lower bounds of the site's metrics

## Response

Who responded, when, and what they did:

- 12:28 — Ethan Hilton (Team Lead) observed the ordering failure in Grafana and began investigation.
- 12:29 — Ethan reproduced the failure via the production UI to gather request/response details.
- 12:30 — Ethan used the browser developer console to inspect failing API responses and located the chaos-test control signal in the response payloads.
- 12:31 — Ethan navigated to the chaos-test control URL and stopped the running chaos test, removing the injected failure condition.
- 12:32 — Ethan validated recovery by placing manual test orders and checking Grafana metrics.
- 13:05 — Ethan created and deployed initial Grafana alerting rules to detect ordering failures and notified the on-call rotation to review alert thresholds.

Obstacles encountered:
- No automated alerts for ordering failures existed, so detection relied on manual dashboard review.
- Chaos test was executed without sufficient production guardrails, complicating diagnosis.

## Root cause

Final root cause analysis:

- A scheduled/triggered chaos test introduced a failure mode that caused the ordering endpoint to return error responses.
- The ordering code contained an unhandled failure path that caused end-to-end order processing to fail when the chaos condition was active.
- Monitoring and alerting did not include checks for ordering success rate or the specific error pattern, so the incident was not automatically paged and went undetected until manual inspection.

## Resolution

How service was restored and how recovery was confirmed:

- The active chaos test was terminated by Ethan Hilton (12:31), which removed the simulated failure condition.
- The ordering endpoint returned to normal operation immediately after the chaos test was stopped.
- Recovery was confirmed by manual test orders and by observing Grafana metrics return to normal (12:32).
- Initial alerting rules were added (13:05) to detect similar failures in the future; these were validated in Grafana.

## Prevention

Planned changes to reduce probability and impact of recurrence:

- Require explicit scheduling, approval, and non-production targets for chaos tests; prohibit ad-hoc chaos tests against production.
- Implement a chaos-test "kill switch" and automated guardrails to stop tests that exceed safe thresholds.
- Fix the ordering endpoint to handle the previously unhandled failure path and add defensive logic (circuit breaker / retries / graceful degradation).
- Add SLOs/SLIs for order success rate and error rate; configure Grafana alerts to page on deviations from SLOs.
- Implement synthetic end-to-end transactions (heartbeat orders) to continuously validate ordering functionality.
- Expand runbooks to include steps for diagnosing and stopping chaos tests and for restoring ordering service.

## Action items

1. Fix ordering endpoint bug and add unit/integration tests that reproduce the chaos-case (owner: Backend team, due: 2025-12-09).
2. Implement chaos-test guardrails: scheduling, approvals, explicit environment flags, and an automated kill switch (owner: DevOps, due: 2025-12-16).
3. Add synthetic end-to-end ordering health checks and configure Grafana alerts to page on deviations (owner: SRE / Ethan Hilton, due: 2025-12-09).
4. Define and publish SLOs/SLIs for order success rate and error rate; add runbooks for ordering incidents (owner: SRE, due: 2025-12-16).
5. Review alerting coverage across critical user flows and tune thresholds; validate alerting via simulated failures (owner: SRE / Ops, due: 2025-12-09).
6. Conduct a post-incident review with the engineering and DevOps teams and update runbooks and deployment policies to prevent unsafe chaos tests (owner: Engineering manager, due: 2025-12-05).
